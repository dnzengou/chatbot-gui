{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0640450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "!conda install tensorflow\n",
    "!conda install keras\n",
    "!conda install pickle5\n",
    "!conda install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3110add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all packages we'll need. \n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import utils as u\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f6ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        #Call tokenizing procedure\n",
    "        w, words, documents, classes, self._intents = self.tokenizing('intents.json')\n",
    "\n",
    "        #Call lemmatizing procedure\n",
    "        w, words, documents, classes, lemmatizer = self.lemmatizing(w, words, documents, classes)\n",
    "\n",
    "        #Call training_data procedure\n",
    "        self._train_x, self._train_y = self.training_data(w, words, documents, classes, lemmatizer)\n",
    "\n",
    "        #Call tokenizing procedure\n",
    "        self._model = self.training(self._train_x, self._train_y)\n",
    "    \n",
    "\n",
    "    def tokenizing(self,url):\n",
    "        words=[]\n",
    "        classes = []\n",
    "        documents = []\n",
    "        intents = json.loads(open(url).read())\n",
    "\n",
    "        for intent in intents['intents']:\n",
    "            for pattern in intent['patterns']:\n",
    "                #tokenize each word\n",
    "                w = nltk.word_tokenize(pattern)\n",
    "                words.extend(w)\n",
    "                #add documents in the corpus\n",
    "                documents.append((w, intent['tag']))\n",
    "                # add to our classes list\n",
    "                if intent['tag'] not in classes:\n",
    "                    classes.append(intent['tag'])\n",
    "\n",
    "        return w, words, documents, classes, intents\n",
    "\n",
    "    def lemmatizing(self, w, words, documents, classes):\n",
    "        ignore_words = ['?', '!']\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "        # lemmatize, lower each word and remove duplicates\n",
    "        words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "\n",
    "        # sort classes and words\n",
    "        classes = sorted(list(set(classes)))\n",
    "        words = sorted(list(set(words)))\n",
    "        # documents = combination between patterns and intents\n",
    "        print (len(documents), \"documents\")\n",
    "\n",
    "        # classes = intents\n",
    "        print (len(classes), \"classes\", classes)\n",
    "\n",
    "        # words = all words, vocabulary\n",
    "        print (len(words), \"unique lemmatized words\", words)\n",
    "\n",
    "        u.create_pickle(words, 'pickles\\words.pkl') \n",
    "        u.create_pickle(classes, 'pickles\\classes.pkl')\n",
    "        return w, words, documents, classes, lemmatizer\n",
    "\n",
    "    def training_data(self, w, words, documents, classes, lemmatizer):\n",
    "        # create our training data\n",
    "        training = []\n",
    "        train_x = []\n",
    "        train_y = []\n",
    "        # create an empty array for our output\n",
    "        output_empty = [0] * len(classes)\n",
    "\n",
    "        # training set, bag of words for each sentence\n",
    "        for doc in documents:\n",
    "            # initialize our bag of words\n",
    "            bag = []\n",
    "            # list of tokenized words for the pattern\n",
    "            pattern_words = doc[0]\n",
    "            # lemmatize each word - create base word, in attempt to represent related words\n",
    "            pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "            # create our bag of words array with 1, if word match found in current pattern\n",
    "\n",
    "            for w in words:\n",
    "                bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "            # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "            output_row = list(output_empty)\n",
    "            output_row[classes.index(doc[1])] = 1\n",
    "            training.append([bag, output_row])\n",
    "\n",
    "        # shuffle our features and turn into np.array\n",
    "        random.shuffle(training)\n",
    "        training = np.array(training)\n",
    "        # create train and test lists. X - patterns, Y - intents\n",
    "        train_x = list(training[:,0])\n",
    "        train_y = list(training[:,1])\n",
    "\n",
    "        print(\"Training data created\")\n",
    "        return train_x, train_y\n",
    "\n",
    "    def training(self,train_x, train_y):\n",
    "        #Sequential from Keras\n",
    "        # Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "        # equal to number of intents to predict output intent with softmax\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "        # Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "        sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "        #fitting and saving the model \n",
    "        hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "        model.save('chatbot_model.h5', hist)\n",
    "        print(\"modeseql created\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_train_x(self):\n",
    "        return self._train_x\n",
    "\n",
    "    def get_train_y(self):\n",
    "        return self._train_y\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self._model\n",
    "\n",
    "    def get_intents(self):\n",
    "        return self._intents\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
